{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn import metrics \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print (pd.__name__, pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text = pd.read_pickle(\"../data/movies.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304354"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender_to</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>gender_from</th>\n",
       "      <th>char_id_from</th>\n",
       "      <th>char_id_to</th>\n",
       "      <th>line_id</th>\n",
       "      <th>words</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L194</td>\n",
       "      <td>we make quick roxanne korrine andrew barrett i...</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L195</td>\n",
       "      <td>well i think we start pronunciation okay you</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L196</td>\n",
       "      <td>hacking gagging spit part please</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L197</td>\n",
       "      <td>okay bout we try french cuisine saturday night</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L198</td>\n",
       "      <td>you ask me cute your name</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender_to movie_id gender_from char_id_from char_id_to line_id  \\\n",
       "0         m       m0           f           u0         u2    L194   \n",
       "1         m       m0           f           u0         u2    L195   \n",
       "2         m       m0           f           u0         u2    L196   \n",
       "3         m       m0           f           u0         u2    L197   \n",
       "4         m       m0           f           u0         u2    L198   \n",
       "\n",
       "                                               words movie_year   genre  \n",
       "0  we make quick roxanne korrine andrew barrett i...       1999  comedy  \n",
       "1       well i think we start pronunciation okay you       1999  comedy  \n",
       "2                   hacking gagging spit part please       1999  comedy  \n",
       "3     okay bout we try french cuisine saturday night       1999  comedy  \n",
       "4                          you ask me cute your name       1999  comedy  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Common n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def grams_df(df, numgrams):\n",
    "\n",
    "    grams_df = pd.DataFrame({'grams': [], 'line_id': []})\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        #print(index)\n",
    "        #create list of bigrams for each text\n",
    "        text = row['words']    \n",
    "        token = nltk.word_tokenize(text)\n",
    "        grams_list = list(ngrams(token, numgrams))\n",
    "        #grams_list = [gram for gram in grams]\n",
    "\n",
    "        #create dict of ngrams\n",
    "        line = row['line_id']\n",
    "        gen = row['gender_from']\n",
    "        d = {'grams': grams_list, 'line_id': np.repeat(line, len(grams_list)), 'gender_from': np.repeat(gen, len(grams_list))}\n",
    "        grams_df = pd.concat([grams_df, pd.DataFrame(d)], axis = 0)\n",
    "        \n",
    "    return grams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test = text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grams_df = grams_df(test,3)\n",
    "grams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bigrams = grams_df(text[:10000],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bigrams = grams_df(text[:50000],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bigrams['grams'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove unknown gender\n",
    "bigrams = bigrams[bigrams['gender_from'] != '?']\n",
    "\n",
    "# get counts\n",
    "grouped = bigrams.groupby(['gender_from', 'grams']).count()\n",
    "g = grouped['line_id'].groupby(level=0, group_keys=False).apply(lambda x: x.sort_values(ascending=False).head(10))\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bigrams_genre = pd.merge(bigrams, text[['genre', 'line_id']], on = 'line_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bigrams_genre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped2 = bigrams_genre.groupby(['genre','gender_from', 'grams']).count()\n",
    "g2 = grouped2['line_id'].groupby(['genre', 'gender_from'], group_keys=False).apply(lambda x: x.sort_values(ascending=False).head(5))\n",
    "g2.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Topic Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#mallet_path = '/Users/jasmindial/Desktop/mallet-2.0.8/bin/mallet'\n",
    "#mallet_path = \"/Users/jasmindial/Desktop/mallet-2.0.8/bin/mallet\"\n",
    "mallet_path = \"../mallet-2.0.8/bin/mallet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove terms with low frequency\n",
    "total = []\n",
    "for index, row in text.iterrows():\n",
    "    text = row['words']    \n",
    "    token = nltk.word_tokenize(text)\n",
    "    total.extend(token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-e1c4ca756a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhigh_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.9\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhigh_freq_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhigh_freq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_freq_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-e1c4ca756a5d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhigh_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.9\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhigh_freq_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhigh_freq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_freq_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "high_freq = nltk.FreqDist(total).most_common(round(.9*text.shape[0]))\n",
    "high_freq_words = [word for word in total if word in high_freq]\n",
    "len(high_freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get two separate texts \n",
    "women = text[text['gender_from'] == 'f']\n",
    "women.name = 'women'\n",
    "men = text[text['gender_from'] == 'm']\n",
    "men.name = 'men'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80720, 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "women.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188200, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "men.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# source: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "# trying the genism version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_ngrams(texts):\n",
    "    \n",
    "    bigram = gensim.models.Phrases(texts, common_terms=stopWords, min_count=5, threshold=50) \n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=15)  \n",
    "\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    \n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stopWords] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prep_for_model(series):\n",
    "    \n",
    "    # convert to list\n",
    "    data = series.values.tolist()\n",
    "\n",
    "    # tokenize \n",
    "    def sent_to_words(sentences):\n",
    "        for sentence in sentences:\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence)))  \n",
    "\n",
    "    data_words = list(sent_to_words(data)) \n",
    "    data_words = add_ngrams(data_words)\n",
    "    \n",
    "    # create dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in data_words]\n",
    "    \n",
    "    return data_words, id2word, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just testing \n",
    "data_words, id2word, corpus = prep_for_model(women.words)\n",
    "# make sure grams are working\n",
    "len(set([word for sent in data_words for word in sent if \"_\" in word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word, num_topics=6, \n",
    "                                                    random_state=100, update_every=1, chunksize=100, passes=10,\n",
    "                                                    alpha='auto', per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#returns gamma, probability of topics for row \n",
    "test = lda_model.inference(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80720"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one per row\n",
    "len(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22.99334526,  27.18163872,  23.92817688,  18.76008797,\n",
       "        20.13619232,  21.81669617], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0][800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# doesn't like this\n",
    "#lda_model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=8, id2word=id2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print (num_topics)\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word, num_topics=num_topics, \n",
    "                                                    random_state=100, update_every=1, chunksize=100, passes=10,\n",
    "                                                    alpha='auto', per_word_topics=True)\n",
    "        #lda_model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(lda_model)\n",
    "        coherencemodel = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# run models and get coherence values\n",
    "# model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus,\n",
    "                                                        #texts=data_words, start=3, limit=13, step=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "women\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "men\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# run topic modeling for both gender scripts \n",
    "# this takes a while :)\n",
    "\n",
    "common_genres = [\"action\", \"drama\", \"comedy\", \"crime\"]\n",
    "\n",
    "topics = []\n",
    "coherence = []\n",
    "\n",
    "for gen in [women, men]:\n",
    "    print (gen.name)\n",
    "    for x in common_genres:\n",
    "        df = gen[gen.genre == x]\n",
    "        group = df.words\n",
    "    \n",
    "        data_words, id2word, corpus = prep_for_model(group)\n",
    "        model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus,\n",
    "                                                            texts=data_words, start=4, limit=13, step=2)\n",
    "        winner = np.argmax(coherence_values)\n",
    "        topics.append(model_list[winner].show_topics())\n",
    "        coherence.append(max(coherence_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.46977957700179357,\n",
       " 0.34450925461689547,\n",
       " 0.36559425716214738,\n",
       " 0.42960439997554917,\n",
       " 0.31859532648701405,\n",
       " 0.42040706275787021,\n",
       " 0.31829103522739638,\n",
       " 0.35003504796796642]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create features\n",
    "\n",
    "for each gender: \n",
    "    for genre in common_genres:\n",
    "        subset the data into right genre\n",
    "        model = d[genre][gender]\n",
    "        gammas = model.inference(subset.words)[0]\n",
    "        concat values to the main dataframe! \n",
    "        \n",
    "    for the rest not in the common_genres:\n",
    "        model = d[overall][gender]\n",
    "        do the same thing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['comedy', 'adventure', 'action', 'crime', 'fantasy', 'sci-fi',\n",
       "       'drama', 'biography', 'horror', 'animation', 'thriller', 'mystery',\n",
       "       'short', 'family', 'romance', 'documentary', 'film-noir'], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# run topic modeling for both gender scripts \n",
    "# this takes a while :)\n",
    "\n",
    "topics = []\n",
    "coherence = []\n",
    "\n",
    "for gen in [women.words, men.words]:\n",
    "    \n",
    "    data_words, id2word, corpus = prep_for_model(gen)\n",
    "    model_list, coherence_values = compute_coherence_values(mallet_path, dictionary=id2word, corpus=corpus,\n",
    "                                                        texts=data_words, start=8, limit=11, step=2)\n",
    "    winner = np.argmax(coherence_values)\n",
    "    topics.append(model_list[winner].show_topics())\n",
    "    coherence.append(max(coherence_values))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "topics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# coherence_values w 5,10,15,20 and w/o removing stop words (10 does best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# coherence_values w 4,8,12,16,20 and w/o removing stop words (8 does best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# trying the sklearn version\n",
    "\n",
    "texts = [men.words, women.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ngram_range = [(1,1), (2,3)] # bag of words, bigrams and trigrams\n",
    "max_features = [1000]\n",
    "no_topics = [8, 10, 15]\n",
    "no_top_words = [5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" ,\".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "# inspiration source: https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# only lda                       \n",
    "\n",
    "def topic_model(texts):\n",
    "\n",
    "    for a in ngram_range:\n",
    "        print (a)\n",
    "        for x in max_features:\n",
    "            print (x)\n",
    "            for y in no_topics: \n",
    "                print (y)\n",
    "                for text in texts: \n",
    "\n",
    "                    #transform\n",
    "                    count_vect = CountVectorizer(ngram_range = a, max_features = x, stop_words = 'english') # using bigrams and trigrams\n",
    "                    word_counts = count_vect.fit_transform(text, )\n",
    "                    tfidf_transformer = TfidfTransformer()\n",
    "                    words_tfidf = tfidf_transformer.fit_transform(word_counts)\n",
    "\n",
    "                    lda = LatentDirichletAllocation(n_topics=y, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(word_counts) \n",
    "                        \n",
    "                    for z in no_top_words: \n",
    "                        print (z)\n",
    "                        display_topics(lda, count_vect.get_feature_names(), z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "topic_model(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# transform\n",
    "count_vect = CountVectorizer(ngram_range = (2,3), max_features = 1000, stop_words = 'english') # using bigrams and trigrams\n",
    "w_words_counts = count_vect.fit_transform(women.words, )\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "w_words_tfidf = tfidf_transformer.fit_transform(w_words_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# transform\n",
    "#count_vect2 = CountVectorizer(ngram_range = (2,3)) # using bigrams and trigrams\n",
    "count_vect2 = CountVectorizer(ngram_range = (2,3), max_features = 1000, stop_words = 'english') # using bigrams and trigrams\n",
    "m_words_counts = count_vect2.fit_transform(men.words, )\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "m_words_tfidf = tfidf_transformer.fit_transform(m_words_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=20, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(w_words_tfidf)\n",
    "nmf2 = NMF(n_components=20, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(m_words_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run LDA for women\n",
    "lda = LatentDirichletAllocation(n_topics=10, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(w_words_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# LDA for men\n",
    "lda2 = LatentDirichletAllocation(n_topics=10, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(m_words_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "no_top_words = 10\n",
    "display_topics(lda, count_vect.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display_topics(lda2, count_vect.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "no_top_words = 10\n",
    "display_topics(nmf2, count_vect2.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# words_tfidf[1,:].toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
