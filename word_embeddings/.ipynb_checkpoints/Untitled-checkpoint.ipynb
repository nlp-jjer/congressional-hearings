{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import gensim, logging\n",
    "import cython\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pickle.load(open(\"../data/movies_small.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender_to</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>gender_from</th>\n",
       "      <th>char_id_from</th>\n",
       "      <th>char_id_to</th>\n",
       "      <th>line_id</th>\n",
       "      <th>words</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L194</td>\n",
       "      <td>we make quick roxanne korrine andrew barrett i...</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L195</td>\n",
       "      <td>well i think we start pronunciation okay you</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L196</td>\n",
       "      <td>hacking gagging spit part please</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L197</td>\n",
       "      <td>okay bout we try french cuisine saturday night</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L198</td>\n",
       "      <td>you ask me cute your name</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender_to movie_id gender_from char_id_from char_id_to line_id  \\\n",
       "0         m       m0           f           u0         u2    L194   \n",
       "1         m       m0           f           u0         u2    L195   \n",
       "2         m       m0           f           u0         u2    L196   \n",
       "3         m       m0           f           u0         u2    L197   \n",
       "4         m       m0           f           u0         u2    L198   \n",
       "\n",
       "                                               words movie_year   genre  \n",
       "0  we make quick roxanne korrine andrew barrett i...       1999  comedy  \n",
       "1       well i think we start pronunciation okay you       1999  comedy  \n",
       "2                   hacking gagging spit part please       1999  comedy  \n",
       "3     okay bout we try french cuisine saturday night       1999  comedy  \n",
       "4                          you ask me cute your name       1999  comedy  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we',\n",
       "  'make',\n",
       "  'quick',\n",
       "  'roxanne',\n",
       "  'korrine',\n",
       "  'andrew',\n",
       "  'barrett',\n",
       "  'incredibly',\n",
       "  'horrendous',\n",
       "  'public',\n",
       "  'break',\n",
       "  'quad'],\n",
       " ['well', 'i', 'think', 'we', 'start', 'pronunciation', 'okay', 'you'],\n",
       " ['hacking', 'gagging', 'spit', 'part', 'please'],\n",
       " ['okay', 'bout', 'we', 'try', 'french', 'cuisine', 'saturday', 'night'],\n",
       " ['you', 'ask', 'me', 'cute', 'your', 'name']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = list(s.split(' ') for s in movies['words'] if s!=\"\")\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-26 13:26:36,117 : INFO : collecting all words and their counts\n",
      "2018-05-26 13:26:36,118 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-05-26 13:26:36,141 : INFO : PROGRESS: at sentence #10000, processed 66935 words, keeping 6555 word types\n",
      "2018-05-26 13:26:36,172 : INFO : PROGRESS: at sentence #20000, processed 134947 words, keeping 9794 word types\n",
      "2018-05-26 13:26:36,205 : INFO : PROGRESS: at sentence #30000, processed 205174 words, keeping 12281 word types\n",
      "2018-05-26 13:26:36,237 : INFO : PROGRESS: at sentence #40000, processed 274660 words, keeping 14378 word types\n",
      "2018-05-26 13:26:36,268 : INFO : PROGRESS: at sentence #50000, processed 338950 words, keeping 16004 word types\n",
      "2018-05-26 13:26:36,307 : INFO : PROGRESS: at sentence #60000, processed 410874 words, keeping 17817 word types\n",
      "2018-05-26 13:26:36,347 : INFO : PROGRESS: at sentence #70000, processed 482188 words, keeping 19217 word types\n",
      "2018-05-26 13:26:36,395 : INFO : PROGRESS: at sentence #80000, processed 554796 words, keeping 20792 word types\n",
      "2018-05-26 13:26:36,456 : INFO : PROGRESS: at sentence #90000, processed 625203 words, keeping 22011 word types\n",
      "2018-05-26 13:26:36,514 : INFO : PROGRESS: at sentence #100000, processed 691723 words, keeping 23443 word types\n",
      "2018-05-26 13:26:36,537 : INFO : PROGRESS: at sentence #110000, processed 761861 words, keeping 24496 word types\n",
      "2018-05-26 13:26:36,598 : INFO : PROGRESS: at sentence #120000, processed 828005 words, keeping 25626 word types\n",
      "2018-05-26 13:26:36,673 : INFO : PROGRESS: at sentence #130000, processed 896778 words, keeping 26566 word types\n",
      "2018-05-26 13:26:36,701 : INFO : PROGRESS: at sentence #140000, processed 967000 words, keeping 27625 word types\n",
      "2018-05-26 13:26:36,752 : INFO : PROGRESS: at sentence #150000, processed 1029235 words, keeping 28383 word types\n",
      "2018-05-26 13:26:36,823 : INFO : PROGRESS: at sentence #160000, processed 1103187 words, keeping 29363 word types\n",
      "2018-05-26 13:26:36,848 : INFO : PROGRESS: at sentence #170000, processed 1172063 words, keeping 30317 word types\n",
      "2018-05-26 13:26:36,929 : INFO : PROGRESS: at sentence #180000, processed 1237383 words, keeping 31288 word types\n",
      "2018-05-26 13:26:36,964 : INFO : PROGRESS: at sentence #190000, processed 1306516 words, keeping 32075 word types\n",
      "2018-05-26 13:26:36,999 : INFO : PROGRESS: at sentence #200000, processed 1376087 words, keeping 32949 word types\n",
      "2018-05-26 13:26:37,131 : INFO : PROGRESS: at sentence #210000, processed 1444637 words, keeping 33867 word types\n",
      "2018-05-26 13:26:37,182 : INFO : PROGRESS: at sentence #220000, processed 1511729 words, keeping 34697 word types\n",
      "2018-05-26 13:26:37,205 : INFO : PROGRESS: at sentence #230000, processed 1578784 words, keeping 35421 word types\n",
      "2018-05-26 13:26:37,309 : INFO : PROGRESS: at sentence #240000, processed 1655241 words, keeping 36314 word types\n",
      "2018-05-26 13:26:37,349 : INFO : PROGRESS: at sentence #250000, processed 1720485 words, keeping 36944 word types\n",
      "2018-05-26 13:26:37,391 : INFO : PROGRESS: at sentence #260000, processed 1791632 words, keeping 37713 word types\n",
      "2018-05-26 13:26:37,450 : INFO : PROGRESS: at sentence #270000, processed 1862141 words, keeping 38501 word types\n",
      "2018-05-26 13:26:37,507 : INFO : PROGRESS: at sentence #280000, processed 1933365 words, keeping 39363 word types\n",
      "2018-05-26 13:26:37,564 : INFO : PROGRESS: at sentence #290000, processed 2010582 words, keeping 39980 word types\n",
      "2018-05-26 13:26:37,589 : INFO : collected 40477 word types from a corpus of 2048050 raw words and 295535 sentences\n",
      "2018-05-26 13:26:37,591 : INFO : Loading a fresh vocabulary\n",
      "2018-05-26 13:26:37,693 : INFO : min_count=3 retains 19362 unique words (47% of original 40477, drops 21115)\n",
      "2018-05-26 13:26:37,694 : INFO : min_count=3 leaves 2021454 word corpus (98% of original 2048050, drops 26596)\n",
      "2018-05-26 13:26:37,808 : INFO : deleting the raw counts dictionary of 40477 items\n",
      "2018-05-26 13:26:37,813 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2018-05-26 13:26:37,815 : INFO : downsampling leaves estimated 1480897 word corpus (73.3% of prior 2021454)\n",
      "2018-05-26 13:26:37,938 : INFO : estimated required memory for 19362 words and 100 dimensions: 25170600 bytes\n",
      "2018-05-26 13:26:37,939 : INFO : resetting layer weights\n",
      "2018-05-26 13:26:38,228 : INFO : training model with 4 workers on 19362 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-05-26 13:26:39,247 : INFO : EPOCH 1 - PROGRESS: at 62.46% examples, 911370 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 13:26:39,857 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-26 13:26:39,860 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-26 13:26:39,873 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-26 13:26:39,874 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-26 13:26:39,877 : INFO : EPOCH - 1 : training on 2048050 raw words (1481388 effective words) took 1.6s, 904077 effective words/s\n",
      "2018-05-26 13:26:40,915 : INFO : EPOCH 2 - PROGRESS: at 58.93% examples, 856207 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 13:26:41,724 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-26 13:26:41,736 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-26 13:26:41,739 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-26 13:26:41,750 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-26 13:26:41,751 : INFO : EPOCH - 2 : training on 2048050 raw words (1481031 effective words) took 1.9s, 800490 effective words/s\n",
      "2018-05-26 13:26:42,778 : INFO : EPOCH 3 - PROGRESS: at 62.57% examples, 908880 words/s, in_qsize 5, out_qsize 2\n",
      "2018-05-26 13:26:43,336 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-26 13:26:43,337 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-26 13:26:43,341 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-26 13:26:43,349 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-26 13:26:43,350 : INFO : EPOCH - 3 : training on 2048050 raw words (1481187 effective words) took 1.6s, 936303 effective words/s\n",
      "2018-05-26 13:26:44,379 : INFO : EPOCH 4 - PROGRESS: at 58.39% examples, 857621 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 13:26:45,045 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-26 13:26:45,052 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-26 13:26:45,053 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-26 13:26:45,060 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-26 13:26:45,060 : INFO : EPOCH - 4 : training on 2048050 raw words (1480805 effective words) took 1.7s, 878977 effective words/s\n",
      "2018-05-26 13:26:46,148 : INFO : EPOCH 5 - PROGRESS: at 51.18% examples, 747884 words/s, in_qsize 4, out_qsize 3\n",
      "2018-05-26 13:26:46,877 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-26 13:26:46,878 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-26 13:26:46,886 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-26 13:26:46,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-26 13:26:46,892 : INFO : EPOCH - 5 : training on 2048050 raw words (1481249 effective words) took 1.8s, 846117 effective words/s\n",
      "2018-05-26 13:26:46,893 : INFO : training on a 10240250 raw words (7405660 effective words) took 8.7s, 854665 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model_all = gensim.models.Word2Vec(sentences, size=100, min_count=3, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7116585056399869"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_all.wv.similarity('french', 'german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-26 13:29:26,227 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_all.wv.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.56960213, -1.1707838 ,  0.18409875,  0.89959586, -1.1007109 ,\n",
       "       -0.90285856,  0.36956534, -0.7394179 ,  0.11521013, -0.1432944 ,\n",
       "       -1.9350374 , -0.3488514 ,  0.9595361 ,  0.31272   , -2.165789  ,\n",
       "       -1.3861431 , -0.16974063,  0.92358387, -1.7694075 ,  0.39801624,\n",
       "        0.8663093 ,  0.24310422, -0.19883105,  1.550204  ,  0.30704924,\n",
       "        0.8890979 ,  0.46502838, -0.5320274 , -0.4498437 ,  0.9007099 ,\n",
       "       -0.3458439 ,  0.45601404,  0.00498245,  0.9931166 ,  1.7974968 ,\n",
       "        0.09367789,  0.9399672 , -0.66112834,  0.62804514, -0.37448964,\n",
       "        0.07266886, -0.43610206, -1.1962409 , -0.5081573 ,  2.7529626 ,\n",
       "        0.94137496,  1.9622893 ,  0.25920877,  0.21766736,  0.6691347 ,\n",
       "        0.15313031, -0.77647936, -0.88550884,  0.7154488 , -0.24715954,\n",
       "        0.23369369, -0.77408683, -0.03117483, -1.8355021 ,  1.826779  ,\n",
       "       -0.39528742,  0.20023367,  0.8254733 , -0.46479437, -0.15346274,\n",
       "        0.6595938 , -1.1449845 , -0.40884578, -0.02597112,  0.08837401,\n",
       "        0.12642273, -0.2797337 ,  0.18169764,  0.28391588,  1.0866425 ,\n",
       "        1.6222209 ,  0.06435393, -0.44140992, -0.5432854 ,  0.02085181,\n",
       "       -0.19074433,  0.04525941, -0.58667624, -0.21402572,  0.5131675 ,\n",
       "       -0.5214739 , -0.18633579, -0.79998237,  0.9017804 ,  0.64708996,\n",
       "       -0.33805457,  1.2163858 ,  1.4204462 ,  0.20842345,  1.1755803 ,\n",
       "        1.0552084 ,  0.2904637 ,  0.56805944,  0.53886586,  0.71916145],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_all.wv['woman']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-26 13:33:09,365 : INFO : saving Word2Vec object under mdl/model_all, separately None\n",
      "2018-05-26 13:33:09,367 : INFO : not storing attribute vectors_norm\n",
      "2018-05-26 13:33:09,368 : INFO : not storing attribute cum_table\n",
      "2018-05-26 13:33:09,627 : INFO : saved mdl/model_all\n"
     ]
    }
   ],
   "source": [
    "model_all.save(\"mdl/model_all\")\n",
    "# To load: model_all = gensim.models.Word2Vec.load(\"mdl/model_all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
