{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import gensim, logging\n",
    "import cython\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pickle.load(open(\"../data/movies_small.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender_to</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>gender_from</th>\n",
       "      <th>char_id_from</th>\n",
       "      <th>char_id_to</th>\n",
       "      <th>line_id</th>\n",
       "      <th>words</th>\n",
       "      <th>movie_year</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L194</td>\n",
       "      <td>we make quick roxanne korrine andrew barrett i...</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L195</td>\n",
       "      <td>well i think we start pronunciation okay you</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L196</td>\n",
       "      <td>hacking gagging spit part please</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L197</td>\n",
       "      <td>okay bout we try french cuisine saturday night</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m</td>\n",
       "      <td>m0</td>\n",
       "      <td>f</td>\n",
       "      <td>u0</td>\n",
       "      <td>u2</td>\n",
       "      <td>L198</td>\n",
       "      <td>you ask me cute your name</td>\n",
       "      <td>1999</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender_to movie_id gender_from char_id_from char_id_to line_id  \\\n",
       "0         m       m0           f           u0         u2    L194   \n",
       "1         m       m0           f           u0         u2    L195   \n",
       "2         m       m0           f           u0         u2    L196   \n",
       "3         m       m0           f           u0         u2    L197   \n",
       "4         m       m0           f           u0         u2    L198   \n",
       "\n",
       "                                               words movie_year   genre  \n",
       "0  we make quick roxanne korrine andrew barrett i...       1999  comedy  \n",
       "1       well i think we start pronunciation okay you       1999  comedy  \n",
       "2                   hacking gagging spit part please       1999  comedy  \n",
       "3     okay bout we try french cuisine saturday night       1999  comedy  \n",
       "4                          you ask me cute your name       1999  comedy  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we',\n",
       "  'make',\n",
       "  'quick',\n",
       "  'roxanne',\n",
       "  'korrine',\n",
       "  'andrew',\n",
       "  'barrett',\n",
       "  'incredibly',\n",
       "  'horrendous',\n",
       "  'public',\n",
       "  'break',\n",
       "  'quad'],\n",
       " ['well', 'i', 'think', 'we', 'start', 'pronunciation', 'okay', 'you'],\n",
       " ['hacking', 'gagging', 'spit', 'part', 'please'],\n",
       " ['okay', 'bout', 'we', 'try', 'french', 'cuisine', 'saturday', 'night'],\n",
       " ['you', 'ask', 'me', 'cute', 'your', 'name']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = list(s.split(' ') for s in movies['words'] if s!=\"\")\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-28 18:09:06,216 : INFO : collecting all words and their counts\n",
      "2018-05-28 18:09:06,220 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-05-28 18:09:06,338 : INFO : PROGRESS: at sentence #10000, processed 66935 words, keeping 6555 word types\n",
      "2018-05-28 18:09:06,445 : INFO : PROGRESS: at sentence #20000, processed 134947 words, keeping 9794 word types\n",
      "2018-05-28 18:09:06,563 : INFO : PROGRESS: at sentence #30000, processed 205174 words, keeping 12281 word types\n",
      "2018-05-28 18:09:06,666 : INFO : PROGRESS: at sentence #40000, processed 274660 words, keeping 14378 word types\n",
      "2018-05-28 18:09:06,738 : INFO : PROGRESS: at sentence #50000, processed 338950 words, keeping 16004 word types\n",
      "2018-05-28 18:09:06,807 : INFO : PROGRESS: at sentence #60000, processed 410874 words, keeping 17817 word types\n",
      "2018-05-28 18:09:06,904 : INFO : PROGRESS: at sentence #70000, processed 482188 words, keeping 19217 word types\n",
      "2018-05-28 18:09:06,965 : INFO : PROGRESS: at sentence #80000, processed 554796 words, keeping 20792 word types\n",
      "2018-05-28 18:09:07,046 : INFO : PROGRESS: at sentence #90000, processed 625203 words, keeping 22011 word types\n",
      "2018-05-28 18:09:07,110 : INFO : PROGRESS: at sentence #100000, processed 691723 words, keeping 23443 word types\n",
      "2018-05-28 18:09:07,191 : INFO : PROGRESS: at sentence #110000, processed 761861 words, keeping 24496 word types\n",
      "2018-05-28 18:09:07,265 : INFO : PROGRESS: at sentence #120000, processed 828005 words, keeping 25626 word types\n",
      "2018-05-28 18:09:07,407 : INFO : PROGRESS: at sentence #130000, processed 896778 words, keeping 26566 word types\n",
      "2018-05-28 18:09:07,473 : INFO : PROGRESS: at sentence #140000, processed 967000 words, keeping 27625 word types\n",
      "2018-05-28 18:09:07,550 : INFO : PROGRESS: at sentence #150000, processed 1029235 words, keeping 28383 word types\n",
      "2018-05-28 18:09:07,853 : INFO : PROGRESS: at sentence #160000, processed 1103187 words, keeping 29363 word types\n",
      "2018-05-28 18:09:07,976 : INFO : PROGRESS: at sentence #170000, processed 1172063 words, keeping 30317 word types\n",
      "2018-05-28 18:09:08,080 : INFO : PROGRESS: at sentence #180000, processed 1237383 words, keeping 31288 word types\n",
      "2018-05-28 18:09:08,210 : INFO : PROGRESS: at sentence #190000, processed 1306516 words, keeping 32075 word types\n",
      "2018-05-28 18:09:08,335 : INFO : PROGRESS: at sentence #200000, processed 1376087 words, keeping 32949 word types\n",
      "2018-05-28 18:09:08,443 : INFO : PROGRESS: at sentence #210000, processed 1444637 words, keeping 33867 word types\n",
      "2018-05-28 18:09:08,540 : INFO : PROGRESS: at sentence #220000, processed 1511729 words, keeping 34697 word types\n",
      "2018-05-28 18:09:08,706 : INFO : PROGRESS: at sentence #230000, processed 1578784 words, keeping 35421 word types\n",
      "2018-05-28 18:09:08,945 : INFO : PROGRESS: at sentence #240000, processed 1655241 words, keeping 36314 word types\n",
      "2018-05-28 18:09:09,038 : INFO : PROGRESS: at sentence #250000, processed 1720485 words, keeping 36944 word types\n",
      "2018-05-28 18:09:09,118 : INFO : PROGRESS: at sentence #260000, processed 1791632 words, keeping 37713 word types\n",
      "2018-05-28 18:09:09,206 : INFO : PROGRESS: at sentence #270000, processed 1862141 words, keeping 38501 word types\n",
      "2018-05-28 18:09:09,306 : INFO : PROGRESS: at sentence #280000, processed 1933365 words, keeping 39363 word types\n",
      "2018-05-28 18:09:09,390 : INFO : PROGRESS: at sentence #290000, processed 2010582 words, keeping 39980 word types\n",
      "2018-05-28 18:09:09,431 : INFO : collected 40477 word types from a corpus of 2048050 raw words and 295535 sentences\n",
      "2018-05-28 18:09:09,433 : INFO : Loading a fresh vocabulary\n",
      "2018-05-28 18:09:09,653 : INFO : min_count=3 retains 19362 unique words (47% of original 40477, drops 21115)\n",
      "2018-05-28 18:09:09,657 : INFO : min_count=3 leaves 2021454 word corpus (98% of original 2048050, drops 26596)\n",
      "2018-05-28 18:09:09,839 : INFO : deleting the raw counts dictionary of 40477 items\n",
      "2018-05-28 18:09:09,843 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2018-05-28 18:09:09,845 : INFO : downsampling leaves estimated 1480897 word corpus (73.3% of prior 2021454)\n",
      "2018-05-28 18:09:10,234 : INFO : estimated required memory for 19362 words and 100 dimensions: 25170600 bytes\n",
      "2018-05-28 18:09:10,244 : INFO : resetting layer weights\n",
      "2018-05-28 18:09:11,005 : INFO : training model with 4 workers on 19362 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-05-28 18:09:12,036 : INFO : EPOCH 1 - PROGRESS: at 27.31% examples, 399796 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-28 18:09:13,056 : INFO : EPOCH 1 - PROGRESS: at 55.40% examples, 401262 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-28 18:09:14,061 : INFO : EPOCH 1 - PROGRESS: at 80.47% examples, 389723 words/s, in_qsize 8, out_qsize 1\n",
      "2018-05-28 18:09:14,736 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-28 18:09:14,746 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-28 18:09:14,755 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-28 18:09:14,770 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-28 18:09:14,772 : INFO : EPOCH - 1 : training on 2048050 raw words (1480438 effective words) took 3.8s, 394732 effective words/s\n",
      "2018-05-28 18:09:15,807 : INFO : EPOCH 2 - PROGRESS: at 23.08% examples, 336447 words/s, in_qsize 8, out_qsize 1\n",
      "2018-05-28 18:09:16,838 : INFO : EPOCH 2 - PROGRESS: at 52.09% examples, 375633 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-28 18:09:17,855 : INFO : EPOCH 2 - PROGRESS: at 78.75% examples, 377857 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-28 18:09:18,583 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-28 18:09:18,591 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-28 18:09:18,619 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-28 18:09:18,624 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-28 18:09:18,626 : INFO : EPOCH - 2 : training on 2048050 raw words (1480931 effective words) took 3.8s, 386745 effective words/s\n",
      "2018-05-28 18:09:19,664 : INFO : EPOCH 3 - PROGRESS: at 25.51% examples, 370119 words/s, in_qsize 6, out_qsize 2\n",
      "2018-05-28 18:09:20,673 : INFO : EPOCH 3 - PROGRESS: at 53.94% examples, 392772 words/s, in_qsize 5, out_qsize 2\n",
      "2018-05-28 18:09:21,685 : INFO : EPOCH 3 - PROGRESS: at 82.86% examples, 401937 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-28 18:09:22,255 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-28 18:09:22,260 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-28 18:09:22,279 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-28 18:09:22,289 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-28 18:09:22,290 : INFO : EPOCH - 3 : training on 2048050 raw words (1480514 effective words) took 3.6s, 406397 effective words/s\n",
      "2018-05-28 18:09:23,306 : INFO : EPOCH 4 - PROGRESS: at 25.87% examples, 383533 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-28 18:09:24,324 : INFO : EPOCH 4 - PROGRESS: at 53.43% examples, 390590 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-28 18:09:25,325 : INFO : EPOCH 4 - PROGRESS: at 81.90% examples, 399624 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-28 18:09:25,953 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-28 18:09:25,969 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-28 18:09:25,988 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-28 18:09:25,994 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-28 18:09:25,996 : INFO : EPOCH - 4 : training on 2048050 raw words (1481482 effective words) took 3.7s, 401380 effective words/s\n",
      "2018-05-28 18:09:27,039 : INFO : EPOCH 5 - PROGRESS: at 25.57% examples, 366479 words/s, in_qsize 7, out_qsize 2\n",
      "2018-05-28 18:09:28,039 : INFO : EPOCH 5 - PROGRESS: at 53.42% examples, 388926 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-28 18:09:29,047 : INFO : EPOCH 5 - PROGRESS: at 81.38% examples, 395259 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-28 18:09:29,666 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-28 18:09:29,684 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-28 18:09:29,708 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-28 18:09:29,713 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-28 18:09:29,715 : INFO : EPOCH - 5 : training on 2048050 raw words (1481119 effective words) took 3.7s, 399878 effective words/s\n",
      "2018-05-28 18:09:29,717 : INFO : training on a 10240250 raw words (7404484 effective words) took 18.7s, 395748 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model_all = gensim.models.Word2Vec(sentences, size=100, min_count=3, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298682953442836"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_all.wv.similarity('fork', 'spoon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-28 18:09:29,756 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_all.wv.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8887823 ,  0.5843662 , -1.4433612 ,  1.6528975 , -1.1323974 ,\n",
       "        1.8598673 , -0.6699696 , -0.79856366, -0.4462283 , -1.124497  ,\n",
       "        0.09772055,  0.06851473, -1.9076995 , -0.49779946, -0.16395466,\n",
       "        1.0017685 , -0.65013546,  1.4275025 , -0.33479625, -0.27996236,\n",
       "       -0.23167257, -0.40326747,  1.4194703 , -0.34521198,  0.534384  ,\n",
       "        0.93979645,  0.8191274 ,  1.8529729 , -1.1244892 , -1.0976552 ,\n",
       "       -0.9274573 ,  0.7849394 , -0.4446491 ,  0.48743358,  0.4943941 ,\n",
       "        0.07061413,  0.9832355 , -0.8781738 , -1.3890562 ,  0.38256395,\n",
       "        0.25277007, -1.1496593 ,  1.4726934 ,  1.1602194 ,  0.5442263 ,\n",
       "       -0.50361717,  1.0379058 , -0.585961  , -0.6740674 ,  0.7058485 ,\n",
       "       -0.3550793 ,  0.22099417,  0.7024282 ,  0.38005868, -0.2126683 ,\n",
       "       -0.7860658 , -0.27896193, -0.6637623 , -1.1320485 ,  0.08148002,\n",
       "        0.80921525,  0.01282703, -1.4499536 ,  1.1806929 , -1.1590331 ,\n",
       "       -0.01213041,  0.6948338 , -1.08083   , -0.6157887 ,  0.7529262 ,\n",
       "       -0.5633625 ,  0.74502397, -1.1093405 , -0.21589926,  0.83594215,\n",
       "       -0.5049735 , -0.68208843,  0.54064786, -0.13868439, -1.3157855 ,\n",
       "        0.6723202 ,  1.6697917 ,  1.4780017 ,  0.9037882 ,  0.03645227,\n",
       "       -0.01892966, -0.2118053 , -0.19534445,  0.70677197,  0.96799904,\n",
       "        0.61529654,  1.0210376 , -0.3555139 ,  0.31547493,  0.79710567,\n",
       "        0.860396  , -0.30528432,  0.37540922,  1.2084614 , -0.03889685],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_all.wv['woman']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-28 18:09:29,885 : INFO : saving Word2Vec object under mdl/model_all, separately None\n",
      "2018-05-28 18:09:29,889 : INFO : not storing attribute vectors_norm\n",
      "2018-05-28 18:09:29,896 : INFO : not storing attribute cum_table\n",
      "2018-05-28 18:09:30,683 : INFO : saved mdl/model_all\n"
     ]
    }
   ],
   "source": [
    "model_all.save(\"mdl/model_all\")\n",
    "# To load: model_all = gensim.models.Word2Vec.load(\"mdl/model_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>action</td>\n",
       "      <td>60933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adventure</td>\n",
       "      <td>10490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>animation</td>\n",
       "      <td>3562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>biography</td>\n",
       "      <td>13131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comedy</td>\n",
       "      <td>69155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>crime</td>\n",
       "      <td>33927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>documentary</td>\n",
       "      <td>1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>drama</td>\n",
       "      <td>71651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>family</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fantasy</td>\n",
       "      <td>4502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>film-noir</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>horror</td>\n",
       "      <td>14021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mystery</td>\n",
       "      <td>2236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>romance</td>\n",
       "      <td>1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sci-fi</td>\n",
       "      <td>1761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>short</td>\n",
       "      <td>924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>thriller</td>\n",
       "      <td>5101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          genre  count\n",
       "0        action  60933\n",
       "1     adventure  10490\n",
       "2     animation   3562\n",
       "3     biography  13131\n",
       "4        comedy  69155\n",
       "5         crime  33927\n",
       "6   documentary   1428\n",
       "7         drama  71651\n",
       "8        family    534\n",
       "9       fantasy   4502\n",
       "10    film-noir    580\n",
       "11       horror  14021\n",
       "12      mystery   2236\n",
       "13      romance   1599\n",
       "14       sci-fi   1761\n",
       "15        short    924\n",
       "16     thriller   5101"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df11=movies.groupby('genre')['words'].apply(lambda x: (x!='').sum()).reset_index(name='count')\n",
    "df11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_drama = movies['words'].loc[(movies['genre'] == 'drama') & (movies['gender_from'] == 'm')]\n",
    "female_drama = movies['words'].loc[(movies['genre'] == 'drama') & (movies['gender_from'] == 'f')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_drama_raw = \" \".join(male_drama)\n",
    "female_drama_raw = \" \".join(female_drama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_action = movies['words'].loc[(movies['genre'] == 'action') & (movies['gender_from'] == 'm')]\n",
    "female_action = movies['words'].loc[(movies['genre'] == 'action') & (movies['gender_from'] == 'f')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_action_raw = \" \".join(male_action)\n",
    "female_action_raw = \" \".join(female_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_comedy = movies['words'].loc[(movies['genre'] == 'comedy') & (movies['gender_from'] == 'm')]\n",
    "female_comedy = movies['words'].loc[(movies['genre'] == 'comedy') & (movies['gender_from'] == 'f')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_comedy_raw = \" \".join(male_comedy)\n",
    "female_comedy_raw = \" \".join(female_comedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_comedy_m0 = movies['words'].loc[(movies['genre'] == 'comedy') & (movies['gender_from'] == 'm') & (movies['movie_id'] == 'm0')]\n",
    "female_comedy_m0 = movies['words'].loc[(movies['genre'] == 'comedy') & (movies['gender_from'] == 'f') & (movies['movie_id'] == 'm0')]\n",
    "male_comedy_m0_raw = \" \".join(male_comedy_m0)\n",
    "female_comedy_m0_raw = \" \".join(female_comedy_m0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dra = TfidfVectorizer()\n",
    "tfidf_dra = v_dra.fit_transform([male_drama_raw , female_drama_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9967581870544995"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tfidf_dra[0], tfidf_dra[1])[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_act = TfidfVectorizer()\n",
    "tfidf_act = v_act.fit_transform([male_action_raw , female_action_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9956236092312521"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tfidf_act[0], tfidf_act[1])[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_com = TfidfVectorizer()\n",
    "tfidf_com = v_com.fit_transform([male_comedy_raw , female_comedy_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9968362064155483"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tfidf_com[0], tfidf_com[1])[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8620607491773777"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_com0 = TfidfVectorizer()\n",
    "tfidf_com0 = v_com.fit_transform([male_comedy_m0_raw , female_comedy_m0_raw])\n",
    "cosine_similarity(tfidf_com0[0], tfidf_com0[1])[0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
